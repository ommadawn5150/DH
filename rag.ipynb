{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# For download embeddings model\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# For embeddings and vector stores\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pipreqs' has no attribute 'init'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpipreqs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pipreqs' has no attribute 'init'"
     ]
    }
   ],
   "source": [
    "pipreqs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model_name = \"intfloat/multilingual-e5-base\"\n",
    "embeddings_path = \"./embedding_model/intfloat/multilingual-e5-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract txts from Website\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "\n",
    "url = 'https://home.cern'\n",
    "response = requests.get(url)\n",
    "soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "links = soup.find_all('a')\n",
    "links = [link.get('href') for link in links if link.get('href') is not None]\n",
    "urls = []\n",
    "for l in links:\n",
    "    if l.startswith('/science') :\n",
    "        if l not in urls:\n",
    "            urls.append(url + l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_community.document_loaders\n",
    "\n",
    "loader = langchain_community.document_loaders.UnstructuredURLLoader(\n",
    "    urls=urls\n",
    ")\n",
    "with open(\"loader.pkl\", \"wb\") as f:\n",
    "    pickle.dump(loader, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"loader.pkl\", \"rb\") as f:\n",
    "    loader = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB establishment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the embeddigns model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists.\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(f\"embedding_model/{embeddings_model_name}/config.json\"):\n",
    "    print(\"Model already exists.\")\n",
    "else:\n",
    "    download_path = snapshot_download(\n",
    "        repo_id=embeddings_model_name,\n",
    "        local_dir = f\"path_to_model/{embeddings_model_name}\",\n",
    "        local_dir_use_symlinks=False # If you want to use symlinks, set this to True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 550, which is longer than the specified 512\n",
      "Created a chunk of size 565, which is longer than the specified 512\n",
      "Created a chunk of size 771, which is longer than the specified 512\n",
      "Created a chunk of size 552, which is longer than the specified 512\n",
      "Created a chunk of size 586, which is longer than the specified 512\n",
      "Created a chunk of size 617, which is longer than the specified 512\n",
      "Created a chunk of size 586, which is longer than the specified 512\n",
      "Created a chunk of size 584, which is longer than the specified 512\n",
      "Created a chunk of size 826, which is longer than the specified 512\n",
      "Created a chunk of size 805, which is longer than the specified 512\n",
      "Created a chunk of size 575, which is longer than the specified 512\n",
      "Created a chunk of size 926, which is longer than the specified 512\n",
      "Created a chunk of size 999, which is longer than the specified 512\n",
      "Created a chunk of size 713, which is longer than the specified 512\n",
      "Created a chunk of size 1203, which is longer than the specified 512\n",
      "Created a chunk of size 699, which is longer than the specified 512\n",
      "Created a chunk of size 1554, which is longer than the specified 512\n",
      "Created a chunk of size 641, which is longer than the specified 512\n",
      "Created a chunk of size 544, which is longer than the specified 512\n",
      "Created a chunk of size 617, which is longer than the specified 512\n",
      "Created a chunk of size 727, which is longer than the specified 512\n",
      "Created a chunk of size 660, which is longer than the specified 512\n",
      "Created a chunk of size 609, which is longer than the specified 512\n",
      "Created a chunk of size 554, which is longer than the specified 512\n",
      "Created a chunk of size 637, which is longer than the specified 512\n",
      "Created a chunk of size 771, which is longer than the specified 512\n",
      "Created a chunk of size 552, which is longer than the specified 512\n",
      "Created a chunk of size 721, which is longer than the specified 512\n",
      "Created a chunk of size 727, which is longer than the specified 512\n",
      "Created a chunk of size 660, which is longer than the specified 512\n",
      "Created a chunk of size 519, which is longer than the specified 512\n",
      "Created a chunk of size 513, which is longer than the specified 512\n",
      "Created a chunk of size 566, which is longer than the specified 512\n",
      "Created a chunk of size 655, which is longer than the specified 512\n",
      "Created a chunk of size 531, which is longer than the specified 512\n",
      "Created a chunk of size 793, which is longer than the specified 512\n",
      "Created a chunk of size 533, which is longer than the specified 512\n",
      "Created a chunk of size 536, which is longer than the specified 512\n",
      "Created a chunk of size 771, which is longer than the specified 512\n",
      "Created a chunk of size 519, which is longer than the specified 512\n",
      "Created a chunk of size 567, which is longer than the specified 512\n",
      "Created a chunk of size 752, which is longer than the specified 512\n",
      "Created a chunk of size 556, which is longer than the specified 512\n",
      "Created a chunk of size 642, which is longer than the specified 512\n",
      "Created a chunk of size 557, which is longer than the specified 512\n",
      "Created a chunk of size 537, which is longer than the specified 512\n",
      "Created a chunk of size 567, which is longer than the specified 512\n",
      "Created a chunk of size 752, which is longer than the specified 512\n",
      "Created a chunk of size 679, which is longer than the specified 512\n",
      "Created a chunk of size 714, which is longer than the specified 512\n",
      "Created a chunk of size 681, which is longer than the specified 512\n",
      "Created a chunk of size 517, which is longer than the specified 512\n",
      "Created a chunk of size 806, which is longer than the specified 512\n",
      "Created a chunk of size 538, which is longer than the specified 512\n",
      "Created a chunk of size 679, which is longer than the specified 512\n",
      "Created a chunk of size 714, which is longer than the specified 512\n",
      "Created a chunk of size 681, which is longer than the specified 512\n",
      "Created a chunk of size 771, which is longer than the specified 512\n",
      "Created a chunk of size 552, which is longer than the specified 512\n",
      "Created a chunk of size 771, which is longer than the specified 512\n",
      "Created a chunk of size 552, which is longer than the specified 512\n",
      "Created a chunk of size 586, which is longer than the specified 512\n",
      "Created a chunk of size 586, which is longer than the specified 512\n",
      "Created a chunk of size 550, which is longer than the specified 512\n",
      "Created a chunk of size 550, which is longer than the specified 512\n",
      "Created a chunk of size 565, which is longer than the specified 512\n",
      "Created a chunk of size 565, which is longer than the specified 512\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 32,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embeddings_path\n",
    ")\n",
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=Chroma,\n",
    "    embedding=embeddings,\n",
    "    text_splitter=text_splitter,\n",
    ").from_loaders([loader])\n",
    "\n",
    "retriever = index.vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template Created\n",
      "input_variables=['context', 'question', 'source'] input_types={} partial_variables={} template=\"\\n        <|system|>\\n        Use the following pieces of context to answer the question at the end. : {context}\\n        And you need to answer with following engagements;\\n            - If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n            - Use markdown formatting when displaying code.\\n            - Emphasis should be used to terminologies.\\n            - Give sources you used at the end.\\n            - Answer in Japanese.\\n        </s>\\n        <|user|>\\n        {question}\\n        </s>\\n        {source}\\n    \"\n"
     ]
    }
   ],
   "source": [
    "from helpers import LLM\n",
    "llm = LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Trained RAG Answer\n",
       "LHCにある実験施設は、9つの異なる実験によって構成されています。 これらの実験は、世界中の研究所の科学者が協力して運営されています。 各実験は、独特の検出器で特徴づけられています。\n",
       "\n",
       "- **ATLAS** (A Toroidal LHC Apparatus): ATLASは、LHCの衝突点で生成される粒子の性質を調べるために設計された検出器です。\n",
       "- **CMS** (Compact Muon Solenoid): CMSは、ATLASと同じく、LHCの衝突点で生成される粒子の性質を調べるために設計された検出器です。\n",
       "- **ALICE** (A Large Ion Collider Experiment): ALICEは、LHCで衝突するイオンの相互作用を調べるために設計された検出器です。\n",
       "- **LHCb** (Large Hadron Collider beauty): LHCbは、LHCの衝突点で生成される美しさ粒子の性質を調べるために設計された検出器です。\n",
       "- **TOTEM** (TOTal cross section, Elastic scattering and diffraction dissociation Measurement at the LHC): TOTEMは、LHCの衝突点でのプロトン-プロトンの弾性散乱とdiffractive dissociationの測定を目的としています。\n",
       "- **LHCf** (Large Hadron Collider forward): LHCfは、LHCの衝突点での粒子の前方散乱の測定を目的としています。\n",
       "- **MoEDAL** (Monopole and Exotics Detector at the LHC): MoEDALは、LHCの衝突点で生成される磁気単極子やexotic particlesの検出を目的としています。\n",
       "- **FASER** (ForwArd Search ExpeRiment): FASERは、LHCの衝突点での粒子の前方散乱の測定を目的としています。\n",
       "- **SHiP** (Search for Hidden Particles): SHiPは、LHCの衝突点で生成される隠れた粒子の検出を目的としています。\n",
       "\n",
       "これらの実験施設は、LHCの衝突点で生成される粒子の性質を調べるために設計されています。\n",
       "\n",
       "Sources:\n",
       "- [CERN - Accelerator Complex](https://home.cern/science/accelerators/accelerator-complex)\n",
       "- [CERN - Experiments](https://home.cern/science/experiments)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"LHCにある実験施設は何ですか？\"\n",
    "response = llm.chat(question, retriever=retriever)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
