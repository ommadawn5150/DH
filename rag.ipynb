{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# For download embeddings model\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# For embeddings and vector stores\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# For langchain_groq\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# For Rag chain\n",
    "from IPython.display import Markdown, display\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings\n",
    "- api_key : api_key\n",
    "- model : put model name here  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('GROQ_API_KEY')\n",
    "model = \"llama-3.1-70b-versatile\"\n",
    "pdf_path = \"./pdf\"\n",
    "embeddings_path = \"./path_to_model/intfloat/multilingual-e5-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract txts from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "# PDFファイルのパスを指定してテキストを取得\n",
    "pdfs = os.listdir(pdf_path)\n",
    "pdf_text = \"\"\n",
    "for pdf in pdfs:\n",
    "    pdf_text += read_text_from_pdf(pdf_path + \"/\" + pdf)\n",
    "\n",
    "with open(\"pdf_text.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pdf_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pdf_text.pkl\", \"rb\") as f:\n",
    "    pdf_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB establishment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the embeddigns model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"intfloat/multilingual-e5-base\"\n",
    "if os.path.isfile(f\"path_to_model/{model_name}/config.json\"):\n",
    "    print(\"Model already exists.\")\n",
    "else:\n",
    "    download_path = snapshot_download(\n",
    "        repo_id=model_name,\n",
    "        local_dir = f\"path_to_model/{model_name}\",\n",
    "        local_dir_use_symlinks=False # ※1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# チャンク間でoverlappingさせながらテキストを分割\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=128,\n",
    ")\n",
    "# テキストを分割\n",
    "splited_text = text_splitter.split_text(pdf_text)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embeddings_path\n",
    ")\n",
    "\n",
    "# テキストを埋め込みベクトルに変換\n",
    "index = FAISS.from_texts(splited_text, embedding=embeddings)\n",
    "# FaissのRetrieverを取得\n",
    "retriever = index.as_retriever(search_kwargs={\"k\": 4})\n",
    "with open(\"retriever.pkl\", \"wb\") as f:\n",
    "    pickle.dump(retriever, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the second run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"retriever.pkl\", \"rb\") as f:\n",
    "    retriever = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.client = ChatGroq(api_key=api_key,)\n",
    "        self.model = \"llama-3.1-70b-versatile\"\n",
    "\n",
    "        self.prompt_template = \"\"\"\n",
    "            <|system|>\n",
    "            Use the following pieces of context to answer the question at the end.\n",
    "            If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "            keep the answer as concise as possible.\n",
    "            Use markdown formatting when displaying code.\n",
    "            Emphasis should be used to terminologies.\n",
    "            You need to provide the sources of the information you provide.\n",
    "\n",
    "            Answer in very easy terms t.\n",
    "            Answer in English.\n",
    "            Then translate the answer into Japanese.\n",
    "\n",
    "            {context}\n",
    "\n",
    "            </s>\n",
    "            <|user|>\n",
    "            {question}\n",
    "            </s>\n",
    "            <|assistant|>\n",
    "\n",
    "        \"\"\"\n",
    "        #  Create the PromptTemplate Instance\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\n",
    "                \"context\",\n",
    "                \"question\",\n",
    "                \"sources\"\n",
    "                ],\n",
    "            template=self.prompt_template,\n",
    "        )\n",
    "\n",
    "        self.llm_chain = self.prompt | self.client | StrOutputParser()\n",
    "\n",
    "    def format_docs(self,docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    def format_sources(self,docs):\n",
    "        \"\"\"\n",
    "        Extracts and formats the sources (metadata) from the retrieved documents.\n",
    "        This function ensures that the source information is passed to the LLM as part of the context.\n",
    "        \"\"\"\n",
    "        # Extract sources (e.g., URLs or document titles) from metadata\n",
    "        sources = [doc.metadata.get('source', 'Unknown Source') for doc in docs]\n",
    "        # Join the sources into a single string to pass to the LLM\n",
    "        return \"Sources:\\n\" + \"\\n\".join(sources)\n",
    "    \n",
    "    def show_markdown(self, markdown_text,title=None):\n",
    "        display(Markdown(title + markdown_text))\n",
    "    \n",
    "    def chat(self, user_input,retriever, show=True):\n",
    "        rag_chain = (\n",
    "            {\n",
    "                \"context\": retriever | self.format_docs,    # Retrieval Step for context\n",
    "                \"sources\": retriever | self.format_sources, # Retrieve and format sources\n",
    "                \"question\": RunnablePassthrough()      # Prompt Generation\n",
    "            }\n",
    "            | self.llm_chain                                # Generation Step\n",
    "        )\n",
    "\n",
    "        response = rag_chain.invoke(user_input)\n",
    "\n",
    "        if show : self.show_markdown(response, title=\"# RAG Answer\\n\")\n",
    "        return response\n",
    "    \n",
    "    def not_fine_tuned_chat(self, user_input, show=True):\n",
    "        response = self.llm_chain.invoke({\"context\": \"\", \"question\": user_input})\n",
    "\n",
    "        if show : self.show_markdown(response, title=\"# RAG Answer\\n\")\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(api_key)\n",
    "query = \"半導体検出器について教えてください。\"\n",
    "response = llm.chat(query,retriever)\n",
    "response = llm.not_fine_tuned_chat(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
